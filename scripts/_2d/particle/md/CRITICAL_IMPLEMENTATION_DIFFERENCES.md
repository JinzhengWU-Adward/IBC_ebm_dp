# PyTorch å®ç°ä¸ IBC å®˜æ–¹çš„å…³é”®å·®å¼‚

## å‘ç°çš„é—®é¢˜

è™½ç„¶é…ç½®å‚æ•°çœ‹èµ·æ¥ä¸€è‡´ï¼Œä½†åœ¨å®ç°ç»†èŠ‚ä¸Šå­˜åœ¨**å…³é”®å·®å¼‚**ï¼Œè¿™äº›å·®å¼‚å¯èƒ½å¯¼è‡´æ•ˆæœæˆªç„¶ä¸åŒã€‚

---

## å·®å¼‚ 1: æ¢¯åº¦ç¬¦å· âš ï¸ **CRITICAL**

### IBC å®˜æ–¹ (`mcmc.py` ç¬¬ 188-191 è¡Œ)

```python
def gradient_wrt_act(energy_network, observations, actions, ...):
    with tf.GradientTape() as g:
        g.watch(actions)
        energies, _ = energy_network((observations, actions), ...)
    # My energy sign is flipped relative to Igor's code,
    # so -1.0 here.
    denergies_dactions = g.gradient(energies, actions) * -1.0  # â† ä¹˜ä»¥ -1.0
    return denergies_dactions, energies
```

**å…³é”®æ³¨é‡Šï¼š"My energy sign is flipped relative to Igor's code, so -1.0 here"**

è¿™æ„å‘³ç€ï¼š
- IBC è®¡ç®—çš„æ˜¯ **`-dE/da`**
- æ›´æ–°æ–¹å‘ï¼š`actions = actions - delta` å…¶ä¸­ `delta = stepsize * (-dE/da + noise)`
- å®é™…ä¸Šæ˜¯ï¼š`actions = actions + stepsize * (dE/da - noise)`

###  ULASampler (`optimizers.py` ç¬¬ 434-456 è¡Œ)

```python
# è®¡ç®—æ¢¯åº¦
grad = torch.autograd.grad(
    energies.sum(),
    samples,
    create_graph=False
)[0]  # â† è¿™æ˜¯ dE/daï¼ˆæ­£æ¢¯åº¦ï¼‰

# ULA æ›´æ–°
noise = torch.randn_like(samples) * self.noise_scale
delta = current_step_size * (0.5 * grad + noise)  # â† ä½¿ç”¨æ­£æ¢¯åº¦
samples = samples - delta  # â† å‡å»delta
```

**åˆ†æï¼š**
- PyTorch è®¡ç®—çš„æ˜¯ **`dE/da`**ï¼ˆæ­£æ¢¯åº¦ï¼‰
- æ›´æ–°ï¼š`samples = samples - stepsize * (0.5 * dE/da + noise)`
- è¿™ä¸ IBC çš„ç¬¦å·æ˜¯**ç›¸åçš„**ï¼

### æ­£ç¡®çš„æ¢¯åº¦æ–¹å‘

**èƒ½é‡åŸºæ¨¡å‹çš„ç›®æ ‡ï¼šæ‰¾åˆ°ä½èƒ½é‡çš„åŠ¨ä½œ**

IBC çš„æ›´æ–°ï¼ˆæŒ‰ç…§ç¬¬190è¡Œçš„æ³¨é‡Šç†è§£ï¼‰ï¼š
```
de_dact = -dE/da  # è´Ÿæ¢¯åº¦
delta = stepsize * (0.5 * (-dE/da) + noise)
actions = actions - delta
      = actions - stepsize * (0.5 * (-dE/da) + noise)
      = actions + stepsize * (0.5 * dE/da - noise)
```

ç­‰ç­‰ï¼Œè¿™é‡Œæœ‰æ­§ä¹‰ã€‚è®©æˆ‘é‡æ–°åˆ†æç¬¬248-259è¡Œï¼š
```python
gradient_scale = 0.5
de_dact = (gradient_scale * l_lambda * de_dact +  # de_dact å·²ç»æ˜¯ -dE/da
           tf.random.normal(tf.shape(actions)) * l_lambda * noise_scale)
delta_actions = stepsize * de_dact
actions = actions - delta_actions  # å‡å» delta
```

æ‰€ä»¥ IBC çš„æ›´æ–°æ˜¯ï¼š
```
de_dact_scaled = 0.5 * (-dE/da) + noise
actions = actions - stepsize * de_dact_scaled
        = actions - stepsize * (0.5 * (-dE/da) + noise)
        = actions + stepsize * (0.5 * dE/da - noise)
```

**è¿™æ„å‘³ç€ IBC æ²¿ç€æ­£æ¢¯åº¦æ–¹å‘ç§»åŠ¨ï¼ˆå¢åŠ èƒ½é‡ï¼‰ï¼ŒåŠ ä¸Šå™ªå£°ï¼**

è¿™çœ‹èµ·æ¥ä¸å¯¹...è®©æˆ‘é‡æ–°ç†è§£ã€‚å®é™…ä¸Šï¼ŒIBC çš„æ³¨é‡Šè¯´"My energy sign is flipped"ï¼Œè¿™å¯èƒ½æ„å‘³ç€ï¼š
- IBC çš„ç½‘ç»œè¾“å‡ºæ˜¯ **è´Ÿèƒ½é‡**ï¼ˆå³ logitsï¼‰
- å› æ­¤ `dE/da` å®é™…ä¸Šæ˜¯ `d(-logits)/da = -d(logits)/da`
- éœ€è¦å†ä¹˜ä»¥ -1 æ¥å¾—åˆ° `d(logits)/da`

**ç»“è®ºï¼šéœ€è¦ä»”ç»†éªŒè¯ SequenceEBM è¾“å‡ºçš„æ˜¯èƒ½é‡è¿˜æ˜¯ logitsï¼**

---

## å·®å¼‚ 2: delta_action_clip âš ï¸ **IMPORTANT**

### IBC å®˜æ–¹ (`mcmc.py` ç¬¬ 236-254 è¡Œ)

```python
def langevin_step(..., delta_action_clip, ...):
    # This effectively scales the gradient as if the actions were
    # in a min-max range of -1 to 1.
    delta_action_clip = delta_action_clip * 0.5*(max_actions - min_actions)  # â† ç¼©æ”¾
    
    unclipped_de_dact = de_dact * 1.0
    grad_norms = compute_grad_norm(grad_norm_type, unclipped_de_dact)
    
    if grad_clip is not None:
        de_dact = tf.clip_by_value(de_dact, -grad_clip, grad_clip)
    
    gradient_scale = 0.5
    de_dact = (gradient_scale * l_lambda * de_dact +
               tf.random.normal(tf.shape(actions)) * l_lambda * noise_scale)
    delta_actions = stepsize * de_dact
    
    # Clip to box.
    delta_actions = tf.clip_by_value(delta_actions, -delta_action_clip,
                                     delta_action_clip)  # â† è£å‰ª delta
    
    actions = actions - delta_actions
    actions = tf.clip_by_value(actions, min_actions, max_actions)
```

**å…³é”®ç‚¹ï¼š**
- `delta_action_clip` é»˜è®¤å€¼ï¼š**0.1**
- å¯¹äº `[-1, 1]` èŒƒå›´ï¼š`delta_action_clip = 0.1 * 0.5 * (1 - (-1)) = 0.1 * 1.0 = 0.1`
- æ¯æ­¥çš„åŠ¨ä½œå˜åŒ–è¢«é™åˆ¶åœ¨ **Â±0.1** èŒƒå›´å†…

### PyTorch ULASampler (`optimizers.py` ç¬¬ 445-462 è¡Œ)

```python
def sample(...):
    # ULA æ›´æ–°ï¼šåŒ¹é… IBC é€»è¾‘
    noise = torch.randn_like(samples) * self.noise_scale
    delta = current_step_size * (0.5 * grad + noise)
    samples = samples - delta
    
    # é™åˆ¶åœ¨è¾¹ç•Œå†…
    samples = samples.clamp(
        min=bounds_tensor[0, :],
        max=bounds_tensor[1, :]
    )  # â† åªè£å‰ªæœ€ç»ˆç»“æœï¼Œæ²¡æœ‰è£å‰ª deltaï¼
```

**å…³é”®ç¼ºå¤±ï¼š**
- âŒ **æ²¡æœ‰ `delta_action_clip`**
- âŒ **æ²¡æœ‰é™åˆ¶æ¯æ­¥çš„å˜åŒ–é‡**
- åªè£å‰ªæœ€ç»ˆä½ç½®åˆ°è¾¹ç•Œå†…

**å½±å“ï¼š**
- PyTorch å®ç°ä¸­ï¼Œæ¯æ­¥å¯èƒ½è·³å¾—å¤ªè¿œï¼ˆç‰¹åˆ«æ˜¯åœ¨åˆå§‹æ­¥é•¿0.1æ—¶ï¼‰
- IBC é€šè¿‡ `delta_action_clip=0.1` ç¡®ä¿æ¯æ­¥æœ€å¤šç§»åŠ¨ 0.1
- è¿™ä¼šå¯¼è‡´é‡‡æ ·è½¨è¿¹å®Œå…¨ä¸åŒï¼

---

## å·®å¼‚ 3: æ¢¯åº¦è£å‰ª âš ï¸

### IBC å®˜æ–¹
```python
if grad_clip is not None:
    de_dact = tf.clip_by_value(de_dact, -grad_clip, grad_clip)
```
- é»˜è®¤ `grad_clip=None`ï¼ˆæ²¡æœ‰æ¢¯åº¦è£å‰ªï¼‰

### PyTorch ULASampler
```python
# æ²¡æœ‰å®ç° grad_clip
```
- âœ… è¿™ä¸ªå·®å¼‚å½±å“è¾ƒå°ï¼ˆå› ä¸ºé»˜è®¤ä¸è£å‰ªï¼‰

---

## å·®å¼‚ 4: å™ªå£°å¤„ç†ç»†èŠ‚

### IBC å®˜æ–¹
```python
l_lambda = 1.0
de_dact = (gradient_scale * l_lambda * de_dact +
           tf.random.normal(tf.shape(actions)) * l_lambda * noise_scale)
```
- `l_lambda = 1.0`ï¼ˆæ’å®šï¼‰
- `gradient_scale = 0.5`
- å™ªå£°ä¹˜ä»¥ `l_lambda * noise_scale = 1.0 * 1.0 = 1.0`

### PyTorch ULASampler
```python
noise = torch.randn_like(samples) * self.noise_scale  # noise_scale = 1.0
delta = current_step_size * (0.5 * grad + noise)
```
- å™ªå£°ç›´æ¥ä¹˜ä»¥ `noise_scale = 1.0`
- âœ… è¿™éƒ¨åˆ†ä¸€è‡´

---

## å·®å¼‚ 5: æ­¥é•¿è°ƒåº¦

### IBC å®˜æ–¹ (`mcmc.py` ç¬¬ 284-295 è¡Œ)
```python
class PolynomialSchedule:
    def __init__(self, init, final, power, total_steps):
        self.init = init
        self.final = final
        self.power = power
        self.total_steps = total_steps
    
    def get_rate(self, step):
        if self.total_steps <= 1:
            return self.init
        progress = tf.minimum(1.0, tf.cast(step, tf.float32) / 
                            tf.cast(self.total_steps - 1, tf.float32))
        return ((self.init - self.final) *
                tf.pow((1.0 - progress), self.power) + self.final)
```

### PyTorch ULASampler (`optimizers.py` ç¬¬ 367-380 è¡Œ)
```python
def _get_step_size(self, step: int) -> float:
    if self.num_steps <= 1:
        return self.step_size
    
    progress = float(step) / float(self.num_steps - 1)
    rate = (self.step_size - self.step_size_final) * (
        (1.0 - progress) ** self.step_size_power
    ) + self.step_size_final
    return rate
```

- âœ… å…¬å¼å®Œå…¨ä¸€è‡´
- âœ… è¾¹ç•Œæ¡ä»¶å¤„ç†ä¸€è‡´

---

## æ€»ç»“ï¼šå…³é”®é—®é¢˜

### ğŸš¨ **æœ€ä¸¥é‡çš„é—®é¢˜**

1. **ç¼ºå°‘ `delta_action_clip`**
   - å½±å“ï¼šæ¯æ­¥å¯èƒ½è·³å¾—å¤ªè¿œï¼Œé‡‡æ ·ä¸ç¨³å®š
   - ä¿®å¤ï¼šæ·»åŠ  `delta_action_clip=0.1` å‚æ•°ï¼Œé™åˆ¶æ¯æ­¥å˜åŒ–é‡

2. **æ¢¯åº¦ç¬¦å·å¯èƒ½ä¸ä¸€è‡´**
   - éœ€è¦éªŒè¯ï¼šSequenceEBM è¾“å‡ºçš„æ˜¯èƒ½é‡è¿˜æ˜¯è´Ÿèƒ½é‡ï¼ˆlogitsï¼‰
   - IBC åœ¨æ¢¯åº¦è®¡ç®—æ—¶ä¹˜ä»¥ -1.0ï¼Œéœ€è¦ç†è§£åŸå› 

### âš ï¸ **æ¬¡è¦é—®é¢˜**

3. **ç¼ºå°‘ `grad_clip`**
   - å½±å“ï¼šè¾ƒå°ï¼ˆé»˜è®¤ä¸ä½¿ç”¨ï¼‰
   - å¯é€‰ä¿®å¤ï¼šæ·»åŠ æ¢¯åº¦è£å‰ªå‚æ•°

---

## ä¿®å¤å»ºè®®

### 1. ä¿®æ”¹ ULASampler æ·»åŠ  delta_action_clip

```python
class ULASampler:
    def __init__(
        self,
        bounds: np.ndarray,
        step_size: float = 0.1,
        num_steps: int = 100,
        noise_scale: float = 1.0,
        step_size_final: float = 1e-5,
        step_size_power: float = 2.0,
        delta_action_clip: float = 0.1,  # â† æ·»åŠ 
        device: str = 'cpu'
    ):
        ...
        self.delta_action_clip = delta_action_clip
    
    def sample(self, ...):
        for step in range(self.num_steps):
            energies = ebm(x, samples)
            grad = torch.autograd.grad(energies.sum(), samples, ...)[0]
            
            # è®¡ç®— delta_action_clipï¼ˆç›¸å¯¹äºåŠ¨ä½œèŒƒå›´ï¼‰
            action_range = bounds_tensor[1, :] - bounds_tensor[0, :]
            delta_clip = self.delta_action_clip * 0.5 * action_range  # â† æ·»åŠ 
            
            with torch.no_grad():
                noise = torch.randn_like(samples) * self.noise_scale
                # æ˜¯å¦éœ€è¦å–è´Ÿæ¢¯åº¦ï¼Ÿéœ€è¦éªŒè¯ï¼
                de_dact = 0.5 * grad + noise  # æˆ– 0.5 * (-grad) + noise
                delta = current_step_size * de_dact
                
                # è£å‰ª deltaï¼ˆå…³é”®ï¼ï¼‰
                delta = delta.clamp(
                    min=-delta_clip,
                    max=delta_clip
                )  # â† æ·»åŠ 
                
                samples = samples - delta
                samples = samples.clamp(
                    min=bounds_tensor[0, :],
                    max=bounds_tensor[1, :]
                )
```

### 2. éªŒè¯æ¢¯åº¦ç¬¦å·

æ£€æŸ¥è®­ç»ƒæŸå¤±æ˜¯å¦æ­£ç¡®ä¸‹é™ï¼š
- å¦‚æœæŸå¤±æŒç»­ä¸Šå‡æˆ–ä¸æ”¶æ•›ï¼Œå¯èƒ½æ˜¯æ¢¯åº¦ç¬¦å·é—®é¢˜
- å°è¯•åœ¨ ULA æ›´æ–°ä¸­ä½¿ç”¨ `-grad` ä»£æ›¿ `grad`

### 3. è°ƒè¯•å»ºè®®

åœ¨è®­ç»ƒå¼€å§‹æ—¶æ‰“å°ï¼š
```python
print(f"ç¬¬ä¸€æ­¥ Langevin é‡‡æ ·:")
print(f"  åˆå§‹èƒ½é‡: {energies[0].mean():.4f}")
print(f"  æœ€ç»ˆèƒ½é‡: {energies[-1].mean():.4f}")
print(f"  èƒ½é‡å˜åŒ–: {(energies[-1] - energies[0]).mean():.4f}")
```

æœŸæœ›ï¼š
- èƒ½é‡åº”è¯¥ä¸‹é™ï¼ˆè´Ÿå˜åŒ–ï¼‰
- å¦‚æœèƒ½é‡ä¸Šå‡ï¼Œè¯´æ˜æ¢¯åº¦æ–¹å‘é”™è¯¯

---

## å®éªŒå»ºè®®

1. **é¦–å…ˆä¿®å¤ `delta_action_clip`**
   - è¿™æ˜¯æœ€æ˜æ˜¾çš„å·®å¼‚
   - åº”è¯¥èƒ½æ˜¾è‘—æ”¹å–„ç¨³å®šæ€§

2. **éªŒè¯æ¢¯åº¦ç¬¦å·**
   - ç›‘æ§ Langevin é‡‡æ ·è¿‡ç¨‹ä¸­çš„èƒ½é‡å˜åŒ–
   - ç¡®è®¤èƒ½é‡æ˜¯ä¸‹é™çš„

3. **å¯¹æ¯”è®­ç»ƒæ›²çº¿**
   - å¦‚æœä¿®å¤åæ•ˆæœä»å·®ï¼Œæ£€æŸ¥å…¶ä»–è®­ç»ƒè¶…å‚æ•°
   - ä¾‹å¦‚ï¼šæ‰¹æ¬¡å¤§å°ã€æ•°æ®å¢å¼ºç­‰

---

## ç»“è®º

**ä¸»è¦é—®é¢˜ä¸åœ¨é…ç½®å‚æ•°ï¼Œè€Œåœ¨å®ç°ç»†èŠ‚ï¼**

å…³é”®å·®å¼‚ï¼š
1. âŒ ç¼ºå°‘ `delta_action_clip`ï¼ˆæ¯æ­¥å˜åŒ–é‡é™åˆ¶ï¼‰
2. âš ï¸ æ¢¯åº¦ç¬¦å·å¯èƒ½ä¸ä¸€è‡´ï¼ˆéœ€è¦éªŒè¯ï¼‰

ä¿®å¤è¿™ä¸¤ä¸ªé—®é¢˜åï¼Œæ•ˆæœåº”è¯¥ä¼šæ˜¾è‘—æ”¹å–„ã€‚

